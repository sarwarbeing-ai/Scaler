{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ANN: Perceptron"
      ],
      "metadata": {
        "id": "_-YIeum0zrqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we understood how we can represent a Logistic Regression model and the linear SVM model as a single-neuron Neural Network.\n",
        "\n",
        "There is another historically important single-neuron model, known as the **Perceptron model**.\n",
        "\n",
        "\n",
        "Suppose we have the same single-neuron structure\n",
        "- Gets $d$ inputs, and one input as $1$ (for bias).\n",
        "- Gives out one output: $o_i$\n",
        "\n",
        "But the activation function is: <br>\n",
        "\\begin{equation}\n",
        "  f_{perceptron}(x_i,w,b)=\\begin{cases}\n",
        "    1, & \\text{if $w^Tx_i+b>0$}.\\\\\n",
        "    0, & \\text{otherwise}.\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "This activation function is called the **Perceptron activation function**\n",
        "\n",
        "\n",
        "**Note:**\n",
        "- Here, we are assuming we already know the weights and bias values. This is a very simple activation function.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What does forward propagation look like in a perceptron?**\n",
        "\n",
        "Here also, it is done in the same way, in 2 steps:-\n",
        "- $z = w_1x_1 + w_2x_2 + ... + w_dx_d + b$\n",
        "- $o_i = f_{perceptron}(z)$\n",
        "\n"
      ],
      "metadata": {
        "id": "qaoFsRaTtAoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 ways of thinking about a model:\n",
        "1. Based on **Equations**\n",
        "2. Based on **Geometry**\n",
        "3. Based on **Neural Network representation**\n",
        "\n",
        "We already saw the equation and NN representation of perceptron.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How is the perceptron model different from the Logistic Regression model?**\n",
        "\n",
        "Recall the geometrical meaning of Logistic Regression model: \n",
        "- The geometry comes from a hyperplane acting as a separator: $Π^d$\n",
        "- And of course, there is **squashing** using the sigmoid function.\n",
        "\n",
        "Coming back to the perceptron. Let's take a closer look at the perceptron activation function.\n",
        "- Given a few positive and negative points, they would be divided by a hyperplane: $Π^d$ which is $w^T.x_i + b = 0$\n",
        " - So this function is simply predicting all datapoints above the hyperplane as positive, and the ones below as negative.\n",
        "- So, here also, a hyperplane is acting as a separator\n",
        "- Only difference is: there is **no squashing** here.\n",
        "- We find that it is a **step function**\n",
        "\n",
        "> **Q. What are some cons of perceptron model?**\n",
        "\n",
        "- These models are even older than LogReg model, and dont perform even as good.\n",
        "- Impact of outliers will be massive, as there is no squashing. So, outliers will just kill this model.\n",
        "- Linear model\n",
        "- Can't get probabilities, only 1 or 0\n",
        "\n",
        "For the same shortcomings, preceptron model is not used as widely anymore. It has been replaced by better activation functions.\n",
        "\n",
        "<br>\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=11tRrA3NlcC9Vy3qHfoHpShhBBxr4vX42)"
      ],
      "metadata": {
        "id": "JPTpOQgI0jQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. What would training look like in a perceptron model?\n",
        "\n",
        "- In this model, we are only getting output values as 0 or 1, and not in form of probabilities.\n",
        " - Hence in order to compute loss, we can use even basic loss functions like **MSE**.\n",
        " - With parameters $w$ and $b$, we need to minimise this loss across all $n$ datapoints.\n",
        "\n",
        "- We can also add **L2 Regularization** on the weight values, to avoid overfitting.\n",
        "\n",
        "This gives us our optimization problem as: \n",
        "$$\\underset{w,b}{min} \\ \\sum_{i-1}^n Loss(y_i,\\hat{y_i}) + \\lambda * L_2Reg \\ (w_j)$$\n",
        "\n",
        "We can easily solve this using **Gradient Descent**.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1H0fOgQZbQDWil0PKq99znvfJgI9s3X_J)\n"
      ],
      "metadata": {
        "id": "yUaQ1PXl-PrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:-**\n",
        "- Neural Networks are also sometimes referred to as **Multi Layered Perceptrons (MLPs)**\n",
        "- Though the name MLP contains \"perceptron\", but the activation function in the neurons here need not be perceptron activation functions. They can be anything.\n",
        "- An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer.\n",
        "- Due to this, in order to avoid any confusion, we will be continuing to use the term Neural network (NN)."
      ],
      "metadata": {
        "id": "5Neh0EVrClaa"
      }
    }
  ]
}