{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Recap**\n",
        "\n",
        "- **MLP: Multi layer Preceptron**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Notations**\n",
        "\n",
        "\n",
        "\n",
        "- **Forward Propagation**\n",
        "\n",
        "\n",
        "- **BackPropagation**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1QkOqtrdAyo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "xF8uZE9SWkcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We made a simple Neural network to classify a  multiclass setting use case"
      ],
      "metadata": {
        "id": "FvAGvcyxXR8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1-f3HjcdZ_K4SeT3P8pr7Bp_zFjJixXXi' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QoOEHBNMtqlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) Question: How many weights did we have for 2 feature input and 3 neurons?\n",
        "\n",
        "Ans: Since we have 2 input and 3 neuron, we have total of 2*3 = 6 weights and 3 bias (one per neuron.)"
      ],
      "metadata": {
        "id": "onTb6K8GDcA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) What activation did we use in output ? \n",
        "\n",
        "Ans: Softmax"
      ],
      "metadata": {
        "id": "9OBG5-3qEBzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we only had a simple network of 3 neuron.\n",
        "\n",
        "#### What if we add more neuron to our network and make it more complex ? \n",
        "\n",
        "Let's see how we do it"
      ],
      "metadata": {
        "id": "miLPyASltzx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP: Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "Nm1EUxZdDsTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make our model/ network complex by adding a layer between our input and output."
      ],
      "metadata": {
        "id": "6SIvTk5_Lmop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1wMWkvsARzV1eN0JpUcgg1J63dJkkcUAD' width=\"800\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "e4kQZtezLt0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what we did here.\n",
        "\n",
        "#### What are we given with?\n",
        "\n",
        "- We know that we have 2 feature input and \n",
        "- we have 3 classes in output.\n",
        "\n"
      ],
      "metadata": {
        "id": "rBy7Ro-wMVPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding the architecture\n",
        "$ $\n",
        "- Here, we use superscript notation to represent the layer.\n",
        "    - So, instead of writing $f_1$, we write it as $f^1_1$ i.e. neuron 1 of layer 1\n",
        "\n",
        "<br>\n",
        "\n",
        "Now,\n",
        "\n",
        "We took our 2 inputs (Layer - 0 / **Input Layer**) and connected them with the the 4 neuron of **Layer-1**\n",
        "- These 4 neurons will be connected with the 3 neuron of the **output layer** (Layer -2).\n",
        "\n",
        "This intermediate layer in between the input and output layer is called **Hidden layer**\n",
        "\n"
      ],
      "metadata": {
        "id": "KQJGCJYeMrvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why is called hidden layer? \n",
        "\n",
        "As both input and output are hidden from this layer, \n",
        "and \n",
        "- we don't directly deal with this layer, \n",
        "- hence, it is called hidden layer.\n",
        "\n",
        "We make our network complex by increasing the number of neuron in the layer or by increasing the number of layers."
      ],
      "metadata": {
        "id": "Ip73Rut6PvmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1G1J430m73eqrBQ0NVCVCHspjaPMpgEba' width=\"800\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "-1geLbxnU3b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What will be the activation function for output layer?\n",
        "\n",
        "Ans: Since it is a multiclass classification problem, we'll use softmax activation function"
      ],
      "metadata": {
        "id": "Rtjvq23hqOAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the activation function for hidden layer ?\n",
        "\n",
        "Recall that we want our model to learn non linear decision boundary, \n",
        "- Hence, we'll always use non linear activation function \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQi26y-Adx8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What happens if each activation function in MLP is a linear function?"
      ],
      "metadata": {
        "id": "922BK2gpmHEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to understand it using an example"
      ],
      "metadata": {
        "id": "W9W8CkSKk_nJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRtHGWr3lJU9"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Case 1:** \n",
        "\n",
        "Given $f(x) = 2x+1, g(x) = 3x+2$, will $f(g(x))$ be linear or non-linear?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1TNF_X-rg9Wee84yURhbc8AfAE0T758f4' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "y4m-x1Y1GkeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = 2(3x+2) + 1 = 6x + 5$ ---> linear\n",
        "\n",
        "- Composition of two linear function is linear. \n",
        "\n",
        "If we use linear activation\n",
        "- it'll become a linear model.\n",
        "\n",
        "Remember, we have to generare higher order non-linear features\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "A4J-JX1JGhCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How non-linear function helps us here ? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Bg08QrImyzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 2:** $f(x) = x^2+1, g(x) = 2x+1$\n",
        "\n"
      ],
      "metadata": {
        "id": "PGSm2cBJGzfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1hZsaEgqvksWKed5pcAtNeGpEWTMexZTA' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "0iLAP0rBHirX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = (2x+1)^2 + 1 = 4x^2 + 2x + 2$ ---> non-linear, higher order\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "l3nTM6vzHhWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. How about we stack non-linearity once more?\n",
        "\n"
      ],
      "metadata": {
        "id": "mPKWUXO4Gvy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3:** $h(x) = x^2+1, f(x) = 2x^2+1, g(x) = x+1$\n"
      ],
      "metadata": {
        "id": "_1fjW79TIxas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1IDG5hLnzrQvKUE0PKgaxr-6_2vcqrWn7' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "IexqaIlRJP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$f(h(g(x))) = 2((x+1)^2+1)^2 + 1 =  2(2x^2 + 2x + 3)^2 + 1$ ---> non-linear, much higher order\n",
        "\n",
        "<br>\n",
        "\n",
        "Conclusion - **Stacking a non-linearity over a linear function, and repeating the process may create complex features** "
      ],
      "metadata": {
        "id": "DNj8pV4eJOpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose we have 2 inputs: $x_1, x_2$\n",
        "\n",
        "They are going through the following **computation graph**.\n",
        "\n",
        "  <center><img src='https://drive.google.com/uc?id=1zVmAdtYjnAiZ0trhE2XV9pSrOw9fFOF9' width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "0gtJg-c_EUNr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57VBrYT0CF7"
      },
      "source": [
        "- $z$ is a weighted sum, so it'll be linear only: $z = 3x_1 + 4x_2$\n",
        "- $a_1$ will be non-linear and complex: $a_1 = 9x_1^2 + 16x_2^2 + 24x_1x_2$\n",
        "- $a_2$ will be non-linear and much more complex: $a_2 = (9x_1^2 + 16x_2^2 + 24x_1x_2 +1)^2 = (9x_1^2 + 16x_2^2)^2 + 2(9x_1^2 + 16x_2^2)(24x_1x_2 +1) + (24x_1x_2 +1)^2$\n",
        "\n",
        "<hr style=\"border:1px solid gray\"> </hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What non-linear function shall we use?  Have you seen any non linear function in the ML algos so far ?\n",
        "\n",
        "\n",
        "Ans: Recall Sigmoid is one of the non linear activation function we saw.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qzxqJcCHipyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: Should we use different activation functions in different layers?\n",
        "\n",
        "***Anwser:***\n",
        "- Theoretically we can. But studies have shown that it is not of much value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cLp_JIjxrU-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notations for N layer NN"
      ],
      "metadata": {
        "id": "RK-cZ904qRhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So, we introduce layer number in notation as superscript \n",
        "\n",
        "Now,\n",
        "As layer increase, earlier weight notation will not work.\n",
        "- So, we add layer number to weight notation as well. "
      ],
      "metadata": {
        "id": "dloOzLLyTN25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For weight: $w^L_{ij}$ \n",
        "\n",
        "where \n",
        "- L represent layer number\n",
        "- and i represents from neuron\n",
        "- j represent to neuron\n",
        "\n",
        "<br>\n",
        "\n",
        "Bias: $b^L_i$\n",
        "where \n",
        "- L represents the layer\n",
        "- i represents the neuron."
      ],
      "metadata": {
        "id": "04JInT7ySCcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1LAnU_9qgGqf9LbK1PzotW6J80WRYb9Rx' width=\"800\"></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX18RkNPbgOI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BY45N9trEWD"
      },
      "source": [
        "\n",
        "\n",
        "We will now have two sets of weights and biases (for the first and second layers) - $W^1, W^2, b^1, b^2$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the dimensions of the parameters $W^1, b^1, W^2, b^2$?\n",
        "\n",
        "\n",
        "Forget about the second layer for now, let's just focus on the first layer, and it's inputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $W^1$\n",
        "- Here, we have **2 inputs** being passed to **4 neurons**\n",
        "- Recall that each pair of input and neuron, has a unique weights value.\n",
        "- Therefore, $W^1$ will be $2\\times4$ matrix:\n",
        "$W^1 = \\begin{bmatrix}\n",
        "w_{11}^1 & w_{12}^1 & w_{13}^1 & w_{14}^1\\\\ \n",
        "w_{21}^1 & w_{22}^1 & w_{23}^1 & w_{24}^1\n",
        "\\end{bmatrix}_{2 \\times 4}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^1$\n",
        "- There are 4 neurons in the first layer.\n",
        "- Each of which has an unique bias value.\n",
        "- Therefore, $b^1$ will be $1 \\times 4$ matrix: $b^1 = \\begin{bmatrix}\n",
        "b_1^1 & b_2^1 & b_3^1 & b_4^1\n",
        "\\end{bmatrix}_{1 \\times 4}$\n"
      ],
      "metadata": {
        "id": "eG79j4RS0j4z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED3RYU5-2GUV"
      },
      "source": [
        "Let's figure dimensions $W^2$ and $b^2$ by considering only layer-1 and layer-2 again as a one layer NN\n",
        "\n",
        "\n",
        "#### $W^2$\n",
        "- Now, the outputs of the layer-1 are inputs to the layer-2.\n",
        "- Layer-2 neurons don't need to know if its an output from an earlier layer, or a raw input\n",
        "- Here, 4 inputs are passed to 3 neurons, $W^2$ will be $4\\times3$ matrix: $W^2 = \\begin{bmatrix}\n",
        "w_{11}^2 & w_{12}^2 & w_{13}^2\\\\ \n",
        "w_{21}^2 & w_{22}^2 & w_{23}^2\\\\ \n",
        "w_{31}^2 & w_{32}^2 & w_{33}^2\\\\ \n",
        "w_{41}^2 & w_{42}^2 & w_{43}^2\n",
        "\\end{bmatrix}_{4 \\times 3}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^2$\n",
        "- There are 3 neurons in second layer, each of which will have a bias associated with them.\n",
        "- Therefore, $b^2$ will be $1 \\times 3$ matrix: $b^2 = \\begin{bmatrix}\n",
        "b_1^2 & b_2^2 & b_3^2\n",
        "\\end{bmatrix}_{1 \\times 3}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define a few variables, that define our NN"
      ],
      "metadata": {
        "id": "p0FXyVnn2-pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# initialize parameters randomly\n",
        "d = 2 # diensionality / number of inputs\n",
        "n = 3 # Number of classes (A/B/C) / Number of neurons in output layer\n",
        "h = 4 # neurons in hidden layer"
      ],
      "metadata": {
        "id": "UpSGuRU0y5oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let;s intitialise these matrices randomly."
      ],
      "metadata": {
        "id": "IHJReVDzyzwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCY9Gn0lrEWE"
      },
      "outputs": [],
      "source": [
        "W1 = 0.01 * np.random.randn(d,h)\n",
        "b1 = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,n)\n",
        "b2 = np.zeros((1,n))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have covered weights and biases, let's talk about forward propagation and backpropagation"
      ],
      "metadata": {
        "id": "F7NIZumEeFHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation"
      ],
      "metadata": {
        "id": "5oxI772Vep1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation is all easy.\n",
        "- We just need to propagation our inputs from left to right\n",
        "- Calculate the value of $z_i$\n",
        "- Apply activation function on top of it\n",
        "- And pass it to neuron in front of it.\n",
        "\n",
        "Ultimately, we'll get the probabilities\n",
        "\n",
        "- Use those probabilities to calcualte the loss.\n",
        "\n",
        "\n",
        "#### Question: What will be the loss here?\n",
        "\n",
        "Ans: Since it is a multiclass classification => **Categorical cross entropy**"
      ],
      "metadata": {
        "id": "z-1htLcpeqUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, comes the tricky part. "
      ],
      "metadata": {
        "id": "LMDaaAKbeqhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation "
      ],
      "metadata": {
        "id": "xT3kKmiYhhuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZLYwdCpdgNYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk \n",
        "df = pd.read_csv(\"/content/spiral.csv\")\n",
        "\n",
        "# Separating feature and label columns\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2DwJZ28gJxB",
        "outputId": "dd683aaa-d66f-46e8-e6af-95b316e759b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk\n",
            "To: /content/spiral.csv\n",
            "\r  0% 0.00/12.9k [00:00<?, ?B/s]\r100% 12.9k/12.9k [00:00<00:00, 12.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's get back to our NN network"
      ],
      "metadata": {
        "id": "KVfSGd_aHqlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1PWWndBIY0xFSxY7E06DEoTH9-JNuZryf' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "bkjAr2U6Hyh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How'll computational graph look for above NN ? "
      ],
      "metadata": {
        "id": "TbKCU6nAIKsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1bqb4oQMnmIDprSb2MSlmDNeJTpJfwW-N' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8P8sGIHoIU44"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NenozJcZrEWE"
      },
      "source": [
        "### Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to calculate $Z^1$,\n",
        "- for this we multiply each row of X with each column of W1\n",
        "- add bias to it (using broadcasting)\n",
        "\n",
        "\n",
        "The formulation comes out to be:\n",
        "\n",
        "$$X.W + b$$"
      ],
      "metadata": {
        "id": "JnXJolbZt133"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2faLFmnrEWE"
      },
      "outputs": [],
      "source": [
        "Z1 = np.dot(X, W1) + b1 # (300,2) x (2,4) + (1,4) => (300,4)\n",
        "A1 = np.maximum(0, Z1) # ReLU if Z1 < 0 A1 =0 else A1 = Z1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculation Z2 and A2"
      ],
      "metadata": {
        "id": "u57B3T81uOXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to get shape of (300,3)\n",
        "- we need to multiply $A^1$ with $W^2$ and add bias $b^2$ to it\n"
      ],
      "metadata": {
        "id": "sBMxydYkx0p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z2 = np.dot(A1, W2) + b2  # (300, 4) x (4, 3) + (1, 3) => (300, 3)\n",
        "# Applying softmax function to get A2\n",
        "Z2_exp = np.exp(Z2)\n",
        "A2 = Z2_exp/np.sum(Z2_exp, axis=1, keepdims=True)\n",
        "probs = A2"
      ],
      "metadata": {
        "id": "d8hQfcg7e8Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC1h07UxrEWE"
      },
      "source": [
        "- Notice that the only change from before is one extra line of code.\n",
        "- We first compute the hidden layer representation and considered that as an input to the output layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=19PU6SUdkaIcJtA6VQbsC1GMAzTMCv_lx' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QOcmp1F8L-7f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLLKODN9rEWE"
      },
      "source": [
        "### Loss\n",
        "\n",
        "#### Question: Will the loss change?\n",
        "\n",
        "No\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "#### Will the gradient calculation change?\n",
        "\n",
        "No, but, we would have to backpropagate the gradients for one additional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djjOv187rEWE"
      },
      "outputs": [],
      "source": [
        "# Number of training examples\n",
        "m = y.shape[0] # 300 datapoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating dz2"
      ],
      "metadata": {
        "id": "KVq2uZLip5t1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=13P54Hx1Jgdd6E5gX2g1q1F8qYIhYzZP6' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "m5MxEogZcIMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$dZ^2 = \\frac{∂L}{∂Z^2}$$\n",
        "\n",
        "\n",
        "So,\n",
        "\n",
        "$$\\frac{∂L}{∂Z^2} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}$$\n",
        "\n",
        "Here, $A^2$ is our output probabilities.\n",
        "- Replaceing $A^2$ with $p$\n",
        "\n",
        "$$\\frac{∂L}{∂Z^2} = \\frac{∂L}{∂p}.\\frac{∂p}{∂Z^2}$$\n",
        "\n",
        "#### Doesn't this look familiar ? \n",
        "\n",
        "- This is similar to what we calculated previously i.e derivative of Loss w.r.t to Z\n",
        "\n",
        "$$dz=\\frac{\\partial J}{\\partial p} \\frac{\\partial p}{\\partial z}$$\n",
        "\n",
        "- The derivative came out to be : $dz = (p_i - I (i=y))$\n",
        "\n",
        "\n",
        "Hence, $dZ_2 = (p_i - I (i=y))$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lNpgXmNpp83F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dZ2 = probs \n",
        "dZ2[range(m),y] -= 1 "
      ],
      "metadata": {
        "id": "FE0y547c5JB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the shape of dZ2 ? "
      ],
      "metadata": {
        "id": "vAA8ofaUysdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As dZ2 is same as probabilties,\n",
        "- its shape will be (m,3) or (300, 3)"
      ],
      "metadata": {
        "id": "ZXfBit3PzAKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating $dW^2$ and $db^2$"
      ],
      "metadata": {
        "id": "Xjyuvgfmr1in"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCwJ4_MPrEWF"
      },
      "source": [
        "Gradient calculation for $dW^2$ and $db^2$ will also be similar to $dW$ and $db$ in the softmax classifier case"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1lB0SMCM2rHK2cTDXXS4bkUFvWaLzXFZ-' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XhaSXlTRtISQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$dW^2 = \\frac{∂L}{∂W^2} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂W^2}$$\n",
        "\n",
        "$$= dZ^2.\\frac{∂Z^2}{∂W^2}$$\n",
        "\n",
        "Here, $Z^2 = W^{2^{T}}.A^1 + b^2$\n",
        "\n",
        "So, $\\frac{∂Z^2}{∂W^2} = A^1$\n",
        "\n",
        "$$dW^2 = \\frac{∂L}{∂W^2} = dZ^2 . A^1$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cCQ1g0TmuBcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to do mat mul for dW2  ? "
      ],
      "metadata": {
        "id": "9x68P15mzLRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Shape of dZ2 = (300, 3)\n",
        "- Shape of A1 = (300, 4)\n",
        "\n",
        "#### What should be the shape of dW2 ?\n",
        "\n",
        "We know that dW2 will be used for updating $W^2$\n",
        "- its shape should match of $W^2$\n",
        "\n",
        "Hence, shape of dW2 will be (4,3)\n",
        "\n"
      ],
      "metadata": {
        "id": "_FVVYHZ81R61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to multipy $dZ^2$ and $A^1$\n",
        "- such that we get (4,3)\n",
        "\n",
        "For that, we take a transpose of A1 and multiply it with dW2.\n",
        "\n",
        "=> $A_1^T. dZ^2$ = (4, 300) x (300, 3) = (4, 3)"
      ],
      "metadata": {
        "id": "XR0kiHkg1lnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1JE6a4iXmFJO_Uno_OlUdccZ6hSMT4mGi' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "95N-MebY1Nwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjY5p67hrEWF"
      },
      "outputs": [],
      "source": [
        "# shape A1 => (300,4)  shape dZ2 (equal to probability)=> (300,3) \n",
        "dW2 = np.dot(A1.T, dZ2)/m # shape => (4, 300) x (300, 3) => (4,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why are we dividing by m ? "
      ],
      "metadata": {
        "id": "sumY4y07_P4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that in GD,\n",
        "- as we are using all datapoints for calculating the updated w\n",
        "    - we take average of it by dividing it by total number of datapoints"
      ],
      "metadata": {
        "id": "gY04HPvNAPy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1d-Oqw5zbGhS3zin_mnycATtK3hOsPDvd' width=\"700\"></center>\n"
      ],
      "metadata": {
        "id": "zGWGgqoP_YEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Where all do we need to divide by m ?\n",
        "$ $ \n",
        "Our goal is to update weights and biases\n",
        "- so we can either do with while calculating the derivates dW2, db2, dW1, db1 (like we are doing)\n",
        "- or we can do it when updating the weights \n",
        "    - i.e. $w^1 = w^1 - η.dw^1.\\frac{1}{m}$"
      ],
      "metadata": {
        "id": "QYpJ9nz9Aelv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we calculate db2"
      ],
      "metadata": {
        "id": "TAmY_3VXt6EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1ZOXIPvQUsnz8OwvooZ6LWTLKutDULdga' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gcoDFQG-wxO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$db^2 = \\frac{∂L}{∂b^2} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂b^2}$$\n",
        "\n",
        "Now,\n",
        "\n",
        "$$\\frac{∂Z^2}{∂b^2} = \\frac{∂(W^2A^1 + b^2)}{db^2} = 1$$\n",
        "\n",
        "$$db^2 = \\frac{∂L}{∂b^2} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.1 = dZ^2$$ \n"
      ],
      "metadata": {
        "id": "Dxg9EqXZw_2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question:  What will be the shape of db2 ? "
      ],
      "metadata": {
        "id": "7fFf3o9T2IP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that db2 will be used to update $b^2$\n",
        "- so their shape should match\n",
        "\n",
        "Hence, Shape of db2 = (1,3)"
      ],
      "metadata": {
        "id": "BP8lb9EWaMGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, dZ2 shape is (300, 3)\n",
        "\n",
        "Now, recall that we are doing GD not SGD.\n",
        "- we need to sum up the derivates across across the rows and then average it out before using it for update\n"
      ],
      "metadata": {
        "id": "u47nIfTPaU4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1ES5B4mTwP6hhAbo1-EZaMkaF4bupg06M' width=\"700\"></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cU9N4RDb43pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- So, we'll take the sum across the row "
      ],
      "metadata": {
        "id": "p_3GhI9rapWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db2 = np.sum(dZ2, axis=0, keepdims=True)/m # shape (1 ,3)"
      ],
      "metadata": {
        "id": "BMAJYaUUt5ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkFcUS5crEWF"
      },
      "source": [
        "However, unlike earlier, we are not done yet, because $A^2$ is also a function of $Z^1$, and indirectly of ($W^1$ and $b^1$)\n",
        "- We still need to calculate $dW^1$ and $db^1$\n",
        "- So, we would need to calculate $dZ^1$, followed by $dW^1$ and $dW^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating $dA^1$"
      ],
      "metadata": {
        "id": "0YJ6aidmx8Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1QLxk1Kaw-ZuEHt3jgUS7Ud7377IWt9kW' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EmTimJIhytPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$dA^1 = \\frac{∂L}{∂A^1} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}$$\n",
        "\n",
        "\n",
        "We know that, $$\\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2} = dZ^2$$\n",
        "\n",
        "Now,\n",
        "\n",
        "$$\\frac{∂Z^2}{∂A^1} = \\frac{∂(W^2A^1 + b^2)}{dA^1} = W^2$$\n",
        "\n",
        "\n",
        "$$dA^1 =\\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.W^2 = dZ^2.W^2$$ \n"
      ],
      "metadata": {
        "id": "Zl0fOfJUzwQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the shape of dA1? \n",
        "\n"
      ],
      "metadata": {
        "id": "w3KWLcxQ5oGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=13wUYyovmm6UTPVJ4asWfWdiBPkcvcf_A' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aFKda7I579ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of $dA^1$ will be same as $A^1$ => (300, 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "9zsyIVge8K6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the shape of $dZ^1$ ? \n",
        "- Same as $Z^1$ => (300, 4)"
      ],
      "metadata": {
        "id": "XlwMMVIVazKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the matrix multiplication  then? "
      ],
      "metadata": {
        "id": "hT9namcB9L1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=14JsUS-VrzKDeVScDMGOhR7b8DRJd6MII' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "WrUBKHlP9vzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that, we have all the shapes\n",
        "- $dZ^2$ = (300, 3)\n",
        "- $W^2$ = (4,3)\n",
        "\n",
        "We need a final shape of (300, 4)\n",
        "\n",
        "So, we will have to \n",
        "- multiply dZ2 with transpose of $W^2$\n",
        "- (300, 3) x (3, 4) => (300, 4)\n",
        "\n",
        "$$dA^1 = dZ^2.W^{2^{T}}$$"
      ],
      "metadata": {
        "id": "CxWtG2Q19RTd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1_fc6GDrEWF"
      },
      "outputs": [],
      "source": [
        "dA1 = np.dot(dZ2, W2.T) # (300,3) x (3 , 4) => (300, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating $dZ^1$"
      ],
      "metadata": {
        "id": "_YXNa9Px0sXd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62HBVo7xrEWF"
      },
      "source": [
        " Now, we have to pass back through the ReLU layer to calculate the gradient $dZ_1$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1A3ZdZS9a-1Ta-1Lk6xYGCVOCDbF0muNs' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lryS3eDq1Zm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{∂L}{∂Z^1} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}.\\frac{∂A^1}{∂Z^1}$$\n",
        "\n",
        "We know that, \n",
        "\n",
        "$$\\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1} = dA^1 $$\n",
        "\n",
        "\n",
        "We have to calculate $\\frac{∂A^1}{∂Z^1}$"
      ],
      "metadata": {
        "id": "yCCb5lqU3QRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1v2agZ0GS-iFu6HtLSTvlJzIU-IjpwtVh' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "dBj0T6DN5cRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1nSSN8JUfFjT2Fne9ioVd7NtvkiumIaXB' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "n-eKtKlq6Dzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyCkJcxRrEWF"
      },
      "outputs": [],
      "source": [
        "dA1[Z1 <= 0] = 0 # was dA1[A1 <= 0] = 0. changed it to dA1[Z1 <= 0] = 0\n",
        "dZ1 = dA1 # same shape as dA1 (300, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why are we updating dA1 and not making a copy of it?\n",
        "\n",
        "Ans: The purpose of calculating dA1 and dZ1 is to ultimately calculate dW1 and db1. \n",
        "\n",
        "These are being used for intermediatory purpose.\n",
        "\n",
        "So, making changes in dA1 won't change anything as \n",
        "- we have already calculated dZ1\n",
        "- and we won't be using dA1 anywhere else except for calc. of dZ1"
      ],
      "metadata": {
        "id": "YVUhW7FHgXYo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiqY9XlQrEWG"
      },
      "source": [
        "This also means that we need to save the intermediate output values from the forward pass. \n",
        "\n",
        "Finally, $dW^1$ and $db^1$ are calculated the same way we did earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating $dW^1$ and $db^1$"
      ],
      "metadata": {
        "id": "3LFF0PybAePt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_XYrgF622SovKGbrcvlP0Aqh5JKECL6U' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "LLIesEgw9DIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1SX20lELZhhIvKS2u4o7E3NsVJaIOpBqN' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "Do9ZfbuJ-3V9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{∂L}{∂W^1} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}.\\frac{∂A^1}{∂Z^1}.\\frac{∂Z^1}{∂W^1}$$\n",
        "\n",
        "We know that, \n",
        "\n",
        "$$\\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}.\\frac{∂Z^1}{∂A^1} = dZ^1 $$\n",
        "\n",
        "\n",
        "We have to calculate $\\frac{∂Z^1}{∂W^1}$\n",
        "\n",
        "$$\\frac{∂Z^1}{∂W^1} = \\frac{∂(W^1.X + b^1)}{∂W^1} = X $$\n",
        "\n",
        "\n",
        "So, \n",
        "\n",
        "$$ \\frac{∂L}{∂W^1} = dZ^1.X$$"
      ],
      "metadata": {
        "id": "0mRg94aC-2DT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, for $db^1$"
      ],
      "metadata": {
        "id": "bA6kxrXi-9Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1eEY30KJ_3NYX-kbhhXYz4dofWKvKWDZQ' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hlLgXqWJAlGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{∂L}{∂b^1} = \\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}.\\frac{∂A^1}{∂Z^1}.\\frac{∂Z^1}{∂b^1}$$\n",
        "\n",
        "We know that, \n",
        "\n",
        "$$\\frac{∂L}{∂A^2}.\\frac{∂A^2}{∂Z^2}.\\frac{∂Z^2}{∂A^1}.\\frac{∂Z^1}{∂A^1} = dZ^1 $$\n",
        "\n",
        "\n",
        "We have to calculate $\\frac{∂Z^1}{∂b^1}$\n",
        "\n",
        "$$\\frac{∂Z^1}{∂b^1} = \\frac{∂(W^1.X + b^1)}{∂b^1} = 1 $$\n",
        "\n",
        "\n",
        "So, \n",
        "\n",
        "$$ \\frac{∂L}{∂b^1} = dZ^1.1$$"
      ],
      "metadata": {
        "id": "RyDoutBHA6hp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjju4gURrEWG"
      },
      "outputs": [],
      "source": [
        "dW1 = np.dot(X.T, dZ1)/m # (2, 300) x (300 ,4) => (2, 4)\n",
        "db1 = np.sum(dZ1, axis=0, keepdims=True)/m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've found these gradients, we update the weight and bias values as:-\n"
      ],
      "metadata": {
        "id": "-TxOQ2KE7uTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-0"
      ],
      "metadata": {
        "id": "DROjJjp16mEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a parameter update\n",
        "W1 += -lr * dW1\n",
        "b1 += -lr * db1\n",
        "W2 += -lr * dW2\n",
        "b2 += -lr * db2"
      ],
      "metadata": {
        "id": "0dqRYm0472sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This parameter updation is done untill it converges (error goes down)."
      ],
      "metadata": {
        "id": "-poceGqg8DIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summarizing whole process"
      ],
      "metadata": {
        "id": "zumaZQ8xSIkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single GD cycle for weight update looks like following :"
      ],
      "metadata": {
        "id": "07kHYWpqYioI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1XfiVgAan0dhRloAFkWr8SLbKMVHk_3ww' width=\"700\"></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KuROszgR5dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write all the derivatives"
      ],
      "metadata": {
        "id": "iiKhmBIKSPiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1YMOO4MkXGq80WsZsptjPPQB1-SeZ-jvF' width=\"500\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "EqWO9qhHRzq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that,\n",
        "- We use dZ2 for calculation of dW2, db2 and dA1.\n",
        "- similarly, we use dA1 for calculation of dZ1.\n",
        "- and dZ1 for calculation of dW1 and db1."
      ],
      "metadata": {
        "id": "trW6kaFfSd8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in order to not calculate value of deeper derivaties i.e dA1, dZ1 again and again\n",
        "- we calculate and store the derivatives of deeper layer\n",
        "- so as we can use them to calculate derivative of shallow layers\n",
        "\n",
        "This is called as **memoization**\n",
        "- also used in **dynamic programming**"
      ],
      "metadata": {
        "id": "9rm0ZvgYS1_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simplfying the single cycle of updation"
      ],
      "metadata": {
        "id": "RGStIXhXZKZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1a7bjDa3R-2e7MShYCbPCqJ-m8R8lJM7_' width=\"700\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "oU8lcUtGWrTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While performing forward prop,\n",
        "- we store/cache the value of $Z^j, W^j, b^j$ in order to use them during back prop\n",
        "- For example: dA1 uses $w^2$ for its calculation."
      ],
      "metadata": {
        "id": "e8uzmQuUYyim"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-nWDh_IrEWG"
      },
      "source": [
        "Done! Let's put everything together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCaMRBxKrEWG",
        "outputId": "978afad0-abdb-4e9a-af70-9ac541aa568b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0: loss 1.098677\n",
            "iteration 1000: loss 0.308916\n",
            "iteration 2000: loss 0.255022\n",
            "iteration 3000: loss 0.251864\n",
            "iteration 4000: loss 0.250284\n",
            "iteration 5000: loss 0.249019\n",
            "iteration 6000: loss 0.248448\n",
            "iteration 7000: loss 0.248166\n",
            "iteration 8000: loss 0.247882\n",
            "iteration 9000: loss 0.247696\n"
          ]
        }
      ],
      "source": [
        "# initialize parameters randomly\n",
        "d = 2\n",
        "h = 100 # size of hidden layer\n",
        "n = 3\n",
        "W1 = 0.01 * np.random.randn(d,h)\n",
        "b1 = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,n)\n",
        "b2 = np.zeros((1,n))\n",
        "\n",
        "# some hyperparameters\n",
        "lr = 1e-0\n",
        "reg = 1e-3 # regularization strength\n",
        "num_examples = X.shape[0]\n",
        "\n",
        "for i in range(10000):\n",
        "\n",
        "    # forward prop\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = np.maximum(0, Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    Z2 = np.exp(Z2)\n",
        "    A2 = Z2 / np.sum(Z2, axis=1, keepdims=True)\n",
        "    probs = A2\n",
        "    \n",
        "    # calc. loss \n",
        "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
        "    data_loss = np.sum(correct_logprobs)/num_examples \n",
        "    reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2) # regularization\n",
        "    loss = data_loss + reg_loss # adding reg. to loss\n",
        "    if i % 1000 == 0:\n",
        "        print(\"iteration %d: loss %f\" % (i, loss))\n",
        "    \n",
        "    # backprop\n",
        "    # compute the gradient on scores\n",
        "    dZ2 = probs\n",
        "    dZ2[range(num_examples),y] -= 1\n",
        "    dZ2 /= num_examples\n",
        "    \n",
        "    # first backprop into parameters W2 and b2\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "    # next backprop into hidden layer, A1\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    # backprop the ReLU non-linearity\n",
        "    dA1[Z1 <= 0] = 0\n",
        "    # finally into W,b\n",
        "    dZ1 = dA1\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "    \n",
        "    # add regularization gradient contribution\n",
        "    dW2 += reg * W2\n",
        "    dW1 += reg * W1\n",
        "    \n",
        "    # perform a parameter update\n",
        "    W1 += -lr * dW1\n",
        "    b1 += -lr * db1\n",
        "    W2 += -lr * dW2\n",
        "    b2 += -lr * db2"
      ]
    }
  ]
}