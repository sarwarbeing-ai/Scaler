{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Boosting Intuition**\n",
        "    - Mathematical intuition\n",
        "    - Comparing Optimization process for boosting vs linear regression\n",
        "    - Example\n",
        "    - Geometric Intuition\n"
      ],
      "metadata": {
        "id": "B4CSZTa6bjAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting"
      ],
      "metadata": {
        "id": "m5pY4W6g6R8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In boosting we do exact opposite of the Bagging technique\n",
        "* In boosting  we have base learners which are typically of **low varaince and high bias** i.e. underfit models\n",
        "\n",
        "####  What sort of DT models are underfit? \n",
        "Ans: Shallow Trees or Decision Stump\n",
        "\n",
        "We combine these model using **Additive Combination** (will be discussed later)\n",
        "\n",
        "Intuitively, we will \n",
        "- Take bunch of simple shallow trees and\n",
        "- add them up to create complex model.\n",
        "  "
      ],
      "metadata": {
        "id": "eF88OXS8eVjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1AQvyQg6Q5lsy5_GCMpuyOgDMiFSTS4-W' >\n",
        "\n"
      ],
      "metadata": {
        "id": "L_7k9FOC6s_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall how we can represent the error in terms of Bias and Variance\n",
        "\n",
        "**Error = Bias$^2$ + Varaince + irreducable error**\n",
        "\n",
        "#### What kind of base learners are we using here? \n",
        "==> Each base learner is a **high bias** and **low variance** model\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the bias/variance for ideal model?\n",
        "==> **Low bias Low variance**\n",
        "\n",
        "But, currently we have high bais and low variance model?\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How do we reduce the biases of the models? \n",
        "\n",
        "Here the idea is to **decrease the bias using Additive Combination** (We'll see how it does that)\n",
        "\n",
        "\n",
        "Recall in RF, we did the exact opposite.\n",
        "- We reduced the variance while keeping bias more or less the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "nKb1-159_RQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1NseAUG7vJIWqm2FIdjuzsMmMyi-lQVOG' >"
      ],
      "metadata": {
        "id": "NezMbdL4__8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting Intuition"
      ],
      "metadata": {
        "id": "7icangb1AAWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to understand boosting using Regression (as it is simpler to understand using regression compared to classification)\n",
        "\n",
        "Assume  we have  \n",
        "- train data set $D_{tr}$ of $n$ points ${(x_i,y_i)}_{i=1}^n$\n",
        "\n",
        "We will built our model in stages\n",
        "\n"
      ],
      "metadata": {
        "id": "WdRlexblide6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Stage 0: $M_0$\n",
        "\n",
        "We build simple **$M_0$** model on our training data **$D_{tr}$ ${(x_i,y_i)}_{i=1}^n$**\n",
        "\n",
        "#### What will be the simplest model for Regression ? \n",
        "Ans: Mean model! \n",
        "- The simplest Regression model is to predict the $ŷ_i$ (predict value) as mean of all $y_i$ (actual value)\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's call this model $h_0(x)$\n",
        "- After calulating the predicted value (ŷ), we find the error for each datapoint\n",
        "\n",
        "<br>\n",
        "\n",
        "#### How do we calculate error? \n",
        "Ans:  $error_i$ = $y_i - ŷ_i$ \n",
        "\n",
        "<br>\n",
        "\n",
        "Since, this error is for the $0^{th}$ model, we write it as \n",
        "- $err_i^0$ = $y_i - ŷ^0_i$ ∀ i = 1 → n \n"
      ],
      "metadata": {
        "id": "uKjidbctli06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1rvJ2pqaOZjJDNQUbSt0OiwhfN9Y5f4D-' >\n"
      ],
      "metadata": {
        "id": "ccN_rcTjvzJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Stage 1: $M_1$\n",
        "\n",
        "In stage 1  we extend the model\n",
        " \n",
        "<br>\n",
        "\n",
        "#### How do we do it?\n",
        "\n",
        "We will build a shallow Decision Tree (can be Decision Stumps in extreme case)\n",
        "\n",
        "\n",
        "The present model $h_1(x)$ (stage 1 model), is trained on the errors of the previous model i.e.\n",
        "- Training data for model 1 is $(x_i, err_i^0)$\n",
        "\n",
        "Now, we want to combine the first and second model \n",
        "\n",
        "<br>\n",
        "\n",
        "#### Question: How do we combine the models?\n",
        "\n",
        "We can do this by summation, but instead of doing simple summation we do weighted summation of the models\n",
        "\n",
        "$F_1(x) = α_0 h_0(x) + α_1 h_1 (x)$\n",
        "\n",
        "where \n",
        "- $F_1(x)$ is the model after stage 1\n",
        "- $h_0(x)$ is model in stage 0\n",
        "- $h_1(x)$ is model in stage 1\n",
        "\n",
        "\n",
        "We find these $α_0, α_1$ using some optimization algorithm (will be discussed later)\n",
        "\n",
        "\n",
        "\n",
        "Now we find the error of the model after stage 1\n",
        "\n",
        "#### Question: What will be the error after stage 1? \n",
        "$error_i^1$ = $y_i -F_1(x_i)$ ∀ $i = 1 → n$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3lk2SX4PW5cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1xWjqkV2X0qbHXEECb93D8dfk5AXLzMNo' >\n"
      ],
      "metadata": {
        "id": "IGYMD7nRxC1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stage 2: $M_2$\n",
        "\n",
        "We again train a shallow Decision tree on ($x_i, error_i^1$) i.e.\n",
        "- training data will be\n",
        "    - $x_i$\n",
        "    - $y_i$'s will be error at stage 1\n",
        "\n",
        "Let this model be $h_2(x)$\n",
        "\n",
        "#### Question: What will be the final model after stage 2?\n",
        "\n",
        "Model after stage 2 becomes:\n",
        " \n",
        "$F_2(x) = α_0 h_0(x) + α_1 h_1 (x) + α_2 h_2(x)$\n",
        "\n",
        "Notice that, \n",
        "\n",
        "we only have to find $α_2$ as other values ($α_1, α_2$) are known"
      ],
      "metadata": {
        "id": "aWCwxsGdYRWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1jD-QlZ8L50TRc04A6cWe5Z7UrmEyFOnb' >\n"
      ],
      "metadata": {
        "id": "Ag1PX2OuxL9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1rlMfGsvfuoubi53Iyp74tNvRC5d_q6dM' >\n"
      ],
      "metadata": {
        "id": "JXTEJs76xlwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stage M\n",
        "\n",
        "If we keep building the models in this way,\n",
        "\n",
        "After $M$ Stages, the final model will be  \n",
        " * $F_M(x) = ∑_{i=0}^M α_i h_i(x)$\n",
        "\n",
        "Here, $h(x)$ is\n",
        "-  either a very simple mean model [for $h_0(x)$]\n",
        "- a shallow tree which are underfit \n",
        "\n",
        "And these weights ($α$'s) are calculated using optimization.\n",
        "\n",
        "#### What is boosting doing intuitively? \n",
        "\n",
        "Intuitively, we are iteratively building our models on the error of the previous models."
      ],
      "metadata": {
        "id": "4NTzVcW8tGiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zrUm835nrv9HtetiMErcszubmP4I9-Vi' >\n"
      ],
      "metadata": {
        "id": "ORUdksiNxyiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing Optimization process for Boosting vs Linear Regression\n",
        "\n",
        "Our model after M stages is $F_M(x) = ∑_{i=0}^M α_i h_i(x)$\n",
        "\n",
        "<br>\n",
        "\n",
        "Recall linear Regression,\n",
        "\n",
        "#### What is the model in Linear Regression?\n",
        "$F(x)= ∑ w_ix_i + w_0$\n",
        "\n",
        "<br>\n",
        "\n",
        "Comparing both, we can say, \n",
        "- Each model $h_i(x)$ is behaving like the features in linear regression ($x_i$) and\n",
        "- $α_i$ is behaving like the weights ($w_i$)\n",
        "\n",
        "\n",
        "In Boosting,\n",
        "-  we build a bunch of simple models and\n",
        "- each model is trained using the residual of previous model. \n",
        "- use these model to build an additive weighted model \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "But there is a difference in the learning process.\n",
        "- In GD for Linear Regression,\n",
        "    - all $w_i$ are learnt at same time \n",
        "- In Boosting,\n",
        "    - each $α_i$ is learnt at different time i.e. at each stage j, we learn an $α_j$\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Do note that,\n",
        "\n",
        "The errors after each stage $j$ ($error_i^j$) are also called **residuals**\n",
        "\n",
        "**Note: These α are also written as γ in some documentations**\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "h-pDt0Vmv7Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1X6UpVSfzD0BQUzaZuo9Xh1fFJgmlx-Ef' >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ROADTiVNx60z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1RIrx_gijux_taDcyJQwfLDbOesvzUoY5' >"
      ],
      "metadata": {
        "id": "caBIba8Svi6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do Note,\n",
        "\n",
        "All the Boosting techniques like XgBoost, AdaBoost and any other boosting techniques are **simple variations or optimizations of the framework** that we learnt"
      ],
      "metadata": {
        "id": "Ly6kWiXX0Sbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1D4b6Gxzjfvu8C4zBtdsBt0KZrOby_X78' >\n"
      ],
      "metadata": {
        "id": "6QL3dRr5zsO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "zKXJRZAbnKR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regression\n",
        "\n",
        "Lets us assume \n",
        "- $x_i$ is a 4 dimensional vector $x$ = [0,1,2,0] and \n",
        "- $y_i$ be 26.2\n",
        "- $γ=1$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "After training ,our first or mean model (i.e $h_0(x_i)$)\n",
        "- it gives out value 12\n",
        "\n",
        "We know $F_m(x_1) = γ_0 h_0(x) + γ_1 h_1 (x) +......+ γ_m h_m(x)$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will the sum of all other model except mean model? \n",
        "The sum of all other models except the mean model should be 14.2. \n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will second model be trained on ? \n",
        "Second model will trained on residual of first i.e 26.2 - 12 = 14.2\n",
        "\n",
        "$h_1(x)$ will be trained on ($x_i, 14.2$)\n",
        "- Suppose it gave out value 4.2\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What is the residual left?\n",
        "26.2 - 10 - 4.2 = 10\n",
        "\n",
        "Similarly, third model will be trained on $x_i$ and residual of previous model\n",
        "\n",
        "In short,\n",
        "\n",
        "- After every iteration as we are training on the residuals of the previous model,  the next models train on lesser and lesser values"
      ],
      "metadata": {
        "id": "0AGOOgQCnOC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1s1FLz84eW4L97hFUka3IOAC0q3Jxe0bE' >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PYRmyxeriNVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification\n",
        "\n",
        "#### How does this work for classification?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZC1XDonQoLtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume \n",
        "- we have the data ($x_i,y_i$)\n",
        "- but our values of $y_i$ are 0 or 1.\n",
        "\n",
        "Now, we have the predict the probability of $y_i$ being 0 or 1 i.e $p_i$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Case 1: $y_i$ = 1\n",
        "For model where $x_i$ = [0,1,3,1] and $y_i=1$ \n",
        "  * let assume $h_0(x_1)$ predicts 0.4\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the error here ? \n",
        "- $error^0$ = 1 - 0.4 = 0.6\n",
        "- Second model will fit the data on ($x_i, 0.6$)\n",
        "- Suppose $h_1(x)$ = 0.2\n",
        "\n",
        "#### What will be final model after stage 2 ? \n",
        "\n",
        "- So, final model after second stage \n",
        "- $F_1(x) = γ_0h_0(x) + y_1h_1(x)$ = 0.4 + 0.2 = 0.6\n",
        "\n",
        "#### What will be residual?\n",
        "\n",
        "Residual will be $y_i - F_1(x)$ = 1 - 0.6 = 0.4\n",
        "\n",
        "\n",
        "We keep on fitting the models on the residual....\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J1LExU4CKB1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1I2c-bxsgVreoPxR04gj46s_G0hcLUODj' >\n"
      ],
      "metadata": {
        "id": "6kHK3t2B1lue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 2: $y_i$ = 0"
      ],
      "metadata": {
        "id": "gj-SPVl6rkgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume the same case,\n",
        "\n",
        "For model where $x_i$ = [0,1,3,1] and $y_i= 0 $ \n",
        "  * let assume $h_0(x_1)$ predicts 0.4\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the residual after $F_0(x)$ or $h_0(x)$ ? \n",
        "\n",
        "Residual = $y_i - F_0(x)$ = 0 - 0.4 = -0.4\n",
        "\n",
        "Now, next model will fit on ($x_i, - 0.4$) and so on .. "
      ],
      "metadata": {
        "id": "0t9cidDVrn3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1LSTGiUi69qIY3_XjEe99mJJdOJcIU5_d' >\n"
      ],
      "metadata": {
        "id": "5pkH1Pli111H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1pjZuf3zDLeY60NHPHOrb-IKYddoNRGSv' >\n",
        "\n"
      ],
      "metadata": {
        "id": "z1C5miib1-yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geometric Interpretation"
      ],
      "metadata": {
        "id": "L-HF7hebSfD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we have training data for single feature $f_1$ (<font color = '#F7CE46'>yellow cross</font>)\n",
        "\n",
        "1. We first build simple model i.e. mean model $h_0(x)$ (<font color ='blue'>blue line</font>)\n",
        "    - We'll get some error $err_i$ (<font color = 'green'>green line</font>)\n",
        "\n",
        "2. We again build a model on ($f_i, err_i^0$) and get model $h_1(x)$.\n",
        "    - We then combine $h_0(x)$ and $h_1(x)$ to get model $F_1(x)$ (<font color ='red'>red line</font>)\n",
        "\n",
        "Notice that \n",
        "the error $err_i^1$ for $F_1(x)$ is less compared to model at stage 0 ($F_0(x)$) \n",
        "\n",
        "We simply continue this process process of model building and additive combining."
      ],
      "metadata": {
        "id": "2WDGTRqc2UxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=15saagbhnuMmTy45yxw5cCG2_8PlmS2l-' >"
      ],
      "metadata": {
        "id": "IITDg4WcqYQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive demonstration: https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html"
      ],
      "metadata": {
        "id": "EmrzGaLxxT0C"
      }
    }
  ]
}