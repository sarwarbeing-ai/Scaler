{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content"
      ],
      "metadata": {
        "id": "ODG1qpw4YjAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hyperparam**\n",
        "    - ccp_alpha\n",
        "\n",
        "- After Ensemble, can we make further improvements? - **Hyper-Parameter tuning**\n",
        "\n",
        "- Can we add more randomness? - **Extra Trees or Extremely Randomised Trees**\n",
        "\n",
        "- Let's test our understanding - Questions\n",
        "\n",
        "- **Summary**"
      ],
      "metadata": {
        "id": "C5RqTo9FYuih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparam"
      ],
      "metadata": {
        "id": "PlS_WQS1ZFYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ccp_alpha- cost complexity pruning**\n",
        "\n",
        "\n",
        "\n",
        "* **PRUNING: Sometimes after we make a tree using the greedy approach and maximising information gain at each step, we eventually end up with some redundant or very less usefull branches. Hence after the tree is completed, we can now go back and merge / remove some paths / subtress inside the tree making it simpler and effecient. This is called Pruning**\n",
        "* This is basicaly used for pruning the base learners \n",
        "* We can control the overfitting and undefitting of the base learners using the value $α$, this is almost similar tp $λ$ whixh we used in linear and logistic regression\n",
        "* so the idea here is to minimise the loss associated with the decision tree and $α$ times the number of terminal nodes (leaves) which controls overfitting \n",
        " * min(loss + $α$ * number of leaves in the tree)\n",
        "* As the depth of tree increases we know the loss decreases, where the number of leaf nodes in increases, this trade-off between the loss and number of leaves can be controlled using $α$ like regularisation."
      ],
      "metadata": {
        "id": "A2hfjQz__RNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1OS3oMsWLqUUzuKEO--J_5dlyDNknjMRl'>\n"
      ],
      "metadata": {
        "id": "h0Sub3dMFMWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1jMy3PfJkOZWRDvLHd2D6fh8FQEcgSpjy'>\n"
      ],
      "metadata": {
        "id": "WpG9Or-GFWQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1JsoH0fNs82BYPKOStZDT4uN2bygFaQbE'>\n"
      ],
      "metadata": {
        "id": "DTdCbMutFl--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyper Parameter Tuning**\n",
        "# **Can we make further improvements?**\n"
      ],
      "metadata": {
        "id": "kRiTtgRT42U6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search\n",
        "\n",
        "**What is GridSearch**\n",
        "- \"In grid search the set of trials is formed by assembling every possible combination of values\"\n",
        "- Basically, trying out every possible combination of hyperparameters in the search space.\n",
        "\n",
        "So we define a parameter space, i.e the range of values for each parameter that we want to try, and then we write a computer program to try out each combination of hyperparamers and we pick the best one.\n",
        "\n",
        "Note that this is a great example where we utilise parallel and even distributed computing at scale"
      ],
      "metadata": {
        "id": "H9hxDB3HHZWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1d_q60Sui-6Txb6LC-XQ2rzbIMpFzHJT8'>"
      ],
      "metadata": {
        "id": "n6hbJEvyU6Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Parametes\n",
        "params = {\n",
        "          'n_estimators' : [100,200,300,400],\n",
        "          'max_depth' : [3,5,10],\n",
        "          'criterion' : ['gini', 'entropy'],\n",
        "          'bootstrap' : [True, False],\n",
        "          'max_features' : [8,9,10]\n",
        "         }"
      ],
      "metadata": {
        "id": "b2cJiP7n5FJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that although we are trying out just a few possibilities for each parameter, in total it becomes many combinations:\n",
        "\n",
        "> **4 * 3 * 2 * 2* 3 = 144**\n",
        "\n",
        "Now we can use SK-Learn to try all these combinations\n"
      ],
      "metadata": {
        "id": "1ID7IsRe6Qnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tuning_function = GridSearchCV(estimator = RandomForestClassifier(), \n",
        "                               param_grid = params,\n",
        "                               scoring = 'accuracy',\n",
        "                               cv = 3,\n",
        "                               n_jobs=-1\n",
        "                               )"
      ],
      "metadata": {
        "id": "foLLZtRy6lPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will fit all combinations, this will take some time to run. (5-6 mins)\n",
        "tuning_function.fit(X_sm, y_sm)\n",
        "\n",
        "parameters = tuning_function.best_params_\n",
        "score = tuning_function.best_score_\n",
        "print(parameters)\n",
        "print(score)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U60eg3h7Hel",
        "outputId": "542eb795-04a9-4384-a268-47c5aaf1353d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bootstrap': False, 'criterion': 'gini', 'max_depth': 10, 'max_features': 8, 'n_estimators': 200}\n",
            "0.9063852813852814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Remeber that Grid Search is a **brute force search**\n",
        "\n",
        "\n",
        "**Why Grid Search?**\n",
        "- Grid Search performs really well for small dimensions of dataset\n",
        "- It's appropriate for small and quick searches of hyperparameter values that are known to perform well generally.\n",
        "- It’s exhaustive and leaves no stone unturned\n",
        "\n",
        "**Cons of Grid Search**\n",
        "- *Curse of Dimensionality*: \n",
        "  - Let's say there are 7 hyperparameters and each hyperparameter has 5 possible values.\n",
        "  - Using Grid Search, no. of combinations tried will be 5 raised to 7, which is 78125 \n",
        "- This exhaustive approach of \"Just try everything\" becomes compute intesive very quickly. \n",
        "- It doesn't work well when the number of hyperparameters is relatively large."
      ],
      "metadata": {
        "id": "rMBPoUjL73he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_validate\n",
        "\n",
        "tree_clf = RandomForestClassifier(random_state=7, bootstrap=False, criterion='gini', max_depth=10, max_features=8, n_estimators=200)\n",
        "kfold = KFold(n_splits=10)\n",
        "cv_acc_results = cross_validate(tree_clf, X_sm, y_sm, cv = kfold, scoring = 'accuracy', return_train_score = True)\n",
        "\n",
        "print(f\"K-Fold Accuracy Mean: Train: {cv_acc_results['train_score'].mean()*100} Validation: {cv_acc_results['test_score'].mean()*100}\")\n",
        "print(f\"K-Fold Accuracy Std: Train: {cv_acc_results['train_score'].std()*100} Validation: {cv_acc_results['test_score'].std()*100}\")"
      ],
      "metadata": {
        "id": "9TkmckqN9FpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc50552-bcce-40d1-aa9b-bcdf9dbd9fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Fold Accuracy Mean: Train: 99.86771618715021 Validation: 91.72767332549941\n",
            "K-Fold Accuracy Std: Train: 0.09621708717668379 Validation: 5.510701433405571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Great, wo do see a significant improvement in performance.**\n",
        "- Train Set: **99.9 %**\n",
        "- Test Set: **92 %**\n",
        "\n",
        "Further notes\n",
        "- Again, this does seem an overfit case, maybe we can reduce max depth a bit. \n",
        "- Grid search can be a great starting point since it eleminates many combinations, we can still do some manual hand-tuning based on domain knowledge of trade-offs, business priorities and required precission/recall levels."
      ],
      "metadata": {
        "id": "obobwlsw9joq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Randomized Search**\n",
        "\n",
        "Just like Grid Search which is an exhaustive brute for search, we can use a Random Search as well, which will try hyper-parameters randomly from a finite list of options, or from a distribution.\n",
        "\n",
        "It can be used when you want to try a hyper-parameter within a certain range with some asssociated probability"
      ],
      "metadata": {
        "id": "mtw0IxHvDTQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=18ZDbe9f-6equ7aOdXSVB046Zcvj1UoUa'>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1MFKfYuLLYD42_IHJd7AtSrKyrCle8Ouf'>"
      ],
      "metadata": {
        "id": "yG1LRsdSVGgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "\n",
        "params = {'ccp_alpha': uniform(loc=0, scale=0.4)}  # sample from uniform dist between 0 to (0+0.4)=0.4\n",
        "\n",
        "tuning_function = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=7, bootstrap=False, criterion='gini', \n",
        "                                                                  max_depth=10, max_features=8, n_estimators=200), \n",
        "                               param_distributions = params,  # notice arg changeed from param_grid to param_distributions\n",
        "                               scoring = 'accuracy',\n",
        "                               cv = 3,\n",
        "                               n_iter=15,  # Number of times to run random combination\n",
        "                               n_jobs=-1\n",
        "                               )\n",
        "\n",
        "tuning_function.fit(X_sm, y_sm)\n",
        "\n",
        "parameters = tuning_function.best_params_\n",
        "score = tuning_function.best_score_\n",
        "print(parameters)\n",
        "print(score)  "
      ],
      "metadata": {
        "id": "4RrqCaBoEDLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2ffda9-8ce5-4362-af92-a265499fa135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ccp_alpha': 0.05604253214986166}\n",
            "0.7159090909090908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here as we can see, we ran a very small randomized seach for just one hyperparmeter, and the score is reduced. So we will move ahead with our original value for *ccp_alpha* which was defaulted to 0\n",
        "\n",
        "**Benifits of Randomized Search**\n",
        "- Hyperparameter space is explored more widely, especially for more important variables\n",
        "- No need to provide an explicit set of possible values for every hyperparameter\n",
        "  - If we provide a good prior (knowlegde of where there is higher chance of finding the best value)\n",
        "- Best configuration can be found in fewer iterations\n",
        "\n",
        "**Short Comings of Random Search**\n",
        "- It does NOT use information from previous experiments to select the next set.\n",
        "- Each next guess is independent of previous experiment"
      ],
      "metadata": {
        "id": "7L4aOec1Isfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coarse to Fine Tuning**\n",
        "\n",
        "As we can see searching for the right hyperparameter is expensive, and it is not possible (or too expensive) in most cases to find the right parameter by using an optimisation framework.\n",
        "\n",
        "So here we can use clever systematic approaches to make our seach slighly more \"guided\".\n",
        "\n",
        "- In coarse to fine approach we first try a wider range (coarse) of a give hyperparameter.\n",
        "- Then, once we have the best one, we try values near to that best value in smaller intervals (Fine).\n",
        "- We can repeat this, until we keep getting reasonable improvements.\n",
        "\n",
        "\n",
        "For example, lets try to tune the n_estimators parameter again"
      ],
      "metadata": {
        "id": "s50jgfBSInGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1JvyopaI7uCCHegzdTd_ST3LvhKKm3AdU'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1i9P88jgM3ivBsG01nreWSxzRewMbx36T'>"
      ],
      "metadata": {
        "id": "cVd7CXg0VZWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'n_estimators': [100,  200, 300, 400, 500]}  # sample from uniform dist between 0 to (0+0.4)=0.4\n",
        "\n",
        "tuning_function = GridSearchCV(estimator = RandomForestClassifier(random_state=7, bootstrap=False, criterion='gini', \n",
        "                                                                  max_depth=10, max_features=8, n_estimators=200), \n",
        "                               param_grid = params,  # notice arg changeed from param_grid to param_distributions\n",
        "                               scoring = 'accuracy',\n",
        "                               cv = 3,\n",
        "                               n_jobs=-1\n",
        "                               )\n",
        "\n",
        "tuning_function.fit(X_sm, y_sm)\n",
        "\n",
        "parameters = tuning_function.best_params_\n",
        "score = tuning_function.best_score_\n",
        "print(parameters)\n",
        "print(score)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18NqOdWcOTcu",
        "outputId": "b1ef8d84-fb92-44e3-eb50-5553b044d9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 400}\n",
            "0.9042207792207794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see 2 things:\n",
        "- Best value for n_estimators is 400\n",
        "- When we used a full grid seach it was 200. This means that hyperparamets cannot be (ideally) tuned independently\n",
        "\n",
        "\n",
        "Now we can look in the vicinity of \"400\" with finer pertebations."
      ],
      "metadata": {
        "id": "bJFOEfH0Owxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'n_estimators': [350, 370, 400, 420, 450]}  # sample from uniform dist between 0 to (0+0.4)=0.4\n",
        "\n",
        "tuning_function = GridSearchCV(estimator = RandomForestClassifier(random_state=7, bootstrap=False, criterion='gini', \n",
        "                                                                  max_depth=10, max_features=8, n_estimators=200), \n",
        "                               param_grid = params,  # notice arg changeed from param_grid to param_distributions\n",
        "                               scoring = 'accuracy',\n",
        "                               cv = 3,\n",
        "                               n_jobs=-1\n",
        "                               )\n",
        "\n",
        "tuning_function.fit(X_sm, y_sm)\n",
        "\n",
        "parameters = tuning_function.best_params_\n",
        "score = tuning_function.best_score_\n",
        "print(parameters)\n",
        "print(score)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWBwZQoYO8hD",
        "outputId": "235050ee-ef04-4929-ab5d-a356f2c33d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 400}\n",
            "0.9042207792207794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that, 400 is the best.\n",
        "- We can try more from 370 to 420 with intervals of say 10.\n",
        "- We can stop here if we are satisfied. (For this lecture we will stop here)"
      ],
      "metadata": {
        "id": "r2bUZek2PSff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "1ydV8CR3wM7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Trees or Extremely Randomised Trees \n"
      ],
      "metadata": {
        "id": "0uxMpWPiGAUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's compare these with Random Forest\n",
        "\n",
        "**What do we do in random forest?**\n",
        "* We do random row sampling and column sampling and then we do aggregation, in which this randomisation and aggregation play a key role in reducing the variace keeping the bias similar, further avoiding overfitting \n",
        "\n",
        "**Randomisation is a great, useful and very powerful strategy for Regularization**\n",
        " "
      ],
      "metadata": {
        "id": "t83SEKWqGKK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DSnJpwOzFCuyQdF5KsUDktJPYZ_szZdv'>\n"
      ],
      "metadata": {
        "id": "imZY7_qTfMoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What do extra trees do?**\n",
        "\n",
        "In extra tree does random row and column sampling and aggregation just like decision trees but it also randomly picks the threshold (τ) to split for numerical features."
      ],
      "metadata": {
        "id": "RwdjbC8ET8mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In decsision trees we saw using the features values as threshold we calculate information gain and then choose the thresold of the split , basing on the values of information gain\n",
        "\n",
        "* In an extra tree we choose this threshold (τ) also randomly ,adding one more randomisation on top of random forest,\n",
        " * say there are $m$ rows in a feature we randomly select few rows and set the threshold basing on their IG values \n",
        "*This is useful when the $m$ is large, but these require more base laerners as the trees are not perfect \n",
        "* Hence these are not widely used"
      ],
      "metadata": {
        "id": "fpwzb_KgUXJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1hLXJNCgbUqp3emjQineZvLf8W6dKfwnV'>\n"
      ],
      "metadata": {
        "id": "FqtL0oSgfXxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1GncvBjGn0p0ZdEYl49SaZxNSgZMIgY1Y'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Ur5-An-Lf0EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n"
      ],
      "metadata": {
        "id": "R9Tgp3M-XzOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if the models have high bias in an ensemble?**\n",
        "\n",
        "* We know Random forest is an ensemble of high varaince models, that is slightly overgfitting models\n",
        "* We see the Gradient Boosted Decision Tree which applies boosting, which deals with high bias or underfitting models"
      ],
      "metadata": {
        "id": "zOhqhojCer9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1nAOfG-cO5tV71OaEZWI4t9CgUBTLPA71'>\n"
      ],
      "metadata": {
        "id": "EFm1D-fN200h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if ensemble a deep decision tree and a simple linear classifier?**\n",
        "\n",
        "* This doesn't work as expected beacuse the random forest expects highy variance models, as it does aggregation at the end which reduces the variancce, but do not effect the bias.Due to which if a model of high bias is there, the high bias remains the same which we should avoid."
      ],
      "metadata": {
        "id": "7pJdBUrJ0whw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1lAQIpDbsaoAvUrcVlRduRdq8xpE4GfCb'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X9YYaDuR3Sdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What happens when $m$ is very close to $n$ ?**\n",
        "* When $m$ is close to $n$ the base learners overfit on the whole train data set. Due to which Aggregation doesn't work in reducing the variance \n",
        "* So, when $\\frac{m}{n}$ increases, Random forest overfits.\n",
        "* But whwn $\\frac{m}{n}$ decreases the base leraners overfit, due to which we require more number of trees for to reduce the variance."
      ],
      "metadata": {
        "id": "5spvniYD3cfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1XABn5maJst8ugowX4fX4FJwYUbw1jYnV'>\n"
      ],
      "metadata": {
        "id": "lxzf8jZN7d7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So here, Should we take weighted average of the base models in aggregation ?**"
      ],
      "metadata": {
        "id": "Wu2VJd2w8RXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this is not required beacuse we are taking Base learners with equal weight \n",
        "\n"
      ],
      "metadata": {
        "id": "uUxbvUqL8ZyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Q5mVqPrdxHw6i88VFZrRAop9pvPz1o86'>\n",
        "\n"
      ],
      "metadata": {
        "id": "kILCvBL59Lit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We saw that aggregation is the key in Random forest ,\n",
        "\n",
        "* But we might have two different types of Base learners in the random forest \n",
        " * With high variance and low bias or\n",
        " * With medium varaince and medium bias \n",
        "* Here as we do aggregation both the models become\n",
        " * with low variance and low bias and \n",
        " * with low variance and medium bias respectively\n",
        "\n",
        "So it's better to have high variance and low bias base learners.\n"
      ],
      "metadata": {
        "id": "ozMID6AKBz1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10eSeKSrmQsNc3jLmqhxLDFxJ7bsETR6X'>"
      ],
      "metadata": {
        "id": "YAp3FGUzDAmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n"
      ],
      "metadata": {
        "id": "h8RJAMXGcntv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Random forest works well when  Dimensionality is not too large\n",
        "* As it is trivially parallelizable, it works very well even when the $n$ is large\n",
        "* The hyper parameters in Random forest are \n",
        " * $k$ : the number of trees\n",
        " * $\\frac{m}{n}$ ratio and\n",
        " \n",
        " * $\\frac{d'}{d}$ ratio\n"
      ],
      "metadata": {
        "id": "Y2TUDmYPcxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Tm-8-_Fnp_V0fpCaqe7CzXjMFRfJ7QY5'>\n"
      ],
      "metadata": {
        "id": "XcOOLaMnf_Cn"
      }
    }
  ]
}